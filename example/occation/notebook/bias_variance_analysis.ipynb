{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#%load_ext line_profiler\n",
    "#%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dic = corpora.Dictionary()\n",
    "tokens = ['']\n",
    "for line in open('data.conll', 'r'):\n",
    "    line = line.strip()\n",
    "    if len(line) == 0:\n",
    "        dic.add_documents([tokens])\n",
    "        del tokens[:]\n",
    "    else:\n",
    "        tokens.extend(line.split()[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sqlearn.crfsuite import crfutils\n",
    "from sqlearn.crfsuite import ner\n",
    "\n",
    "def load_data(f):\n",
    "    X = []\n",
    "    y = []\n",
    "    sent = []\n",
    "    sent_label = []\n",
    "    for line in codecs.open(f, 'r', 'utf-8'):\n",
    "        line = line.strip('\\n')\n",
    "        if line.strip() == '':\n",
    "            sent.append('\\n')\n",
    "            for item in crfutils.readiter(sent, ['w', 'pos'], ' ', dic.token2id):\n",
    "                ner.feature_extractor(item)\n",
    "                X.append(item)\n",
    "                y.append(sent_label)\n",
    "            sent = []\n",
    "            sent_label = []\n",
    "        else:\n",
    "            splited_line = line.split(' ')\n",
    "            sent.append('%s %s' % (splited_line[0], splited_line[1]))\n",
    "            sent_label.append(splited_line[2])\n",
    "\n",
    "    X = [[feature['F'] for feature in sent] for sent in X]\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    print(len(X))\n",
    "    print(len(y))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X, y = load_data('data.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%writefile prof.py\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import pycrfsuite as crf\n",
    "\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "trainer = crf.Trainer(verbose=False)\n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "    'num_memories': 3,\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "tagger = crf.Tagger()\n",
    "    \n",
    "def error_score(y_true, y_pred):\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "\n",
    "#    tagset = set(lb.classes_) - {'O'}\n",
    "#    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    y_acc_eval_array = y_true_combined + y_pred_combined\n",
    "    error_score = {item[0]: 1 - sum(y_acc_eval_array[:, item[1]] == 2)/sum(y_acc_eval_array[:, item[1]] > 0) for item in class_indices.items()}\n",
    "    \n",
    "    return error_score\n",
    "\n",
    "def measure(data_size, X, y):\n",
    "    train_errors = defaultdict(list)\n",
    "    test_errors = defaultdict(list)\n",
    "    \n",
    "    kf = cross_validation.KFold(n=data_size, n_folds=3, shuffle=True, random_state=None)\n",
    "    \n",
    "    for fold_idx, (train_index, test_index) in enumerate(kf):\n",
    "        print('Iteration #%i' % fold_idx)\n",
    "        \n",
    "        X_train, y_train = X[train_index], y[train_index]\n",
    "        X_test, y_test = X[test_index], y[test_index]\n",
    "\n",
    "        # train\n",
    "        model_name = 'm%i.crfsuite' % fold_idx\n",
    "        Xy = zip(X_train, y_train)\n",
    "        for xseq, yseq in Xy:\n",
    "            trainer.append(xseq, yseq)\n",
    "        trainer.train(model_name)\n",
    "        trainer.clear()\n",
    "        \n",
    "        # predict\n",
    "\n",
    "        tagger.open(model_name)\n",
    "        y_train_pred = [tagger.tag(xseq) for xseq in X_train]\n",
    "        y_test_pred  = [tagger.tag(xseq) for xseq in X_test]\n",
    "        tagger.close()\n",
    "        \n",
    "        # evaluate\n",
    "        train_error = error_score(y_train, y_train_pred)\n",
    "        test_error = error_score(y_test, y_test_pred)\n",
    "        \n",
    "        map(lambda item: train_errors[item[0]].append(item[1]), train_error.items())\n",
    "        map(lambda item: test_errors[item[0]].append(item[1]), test_error.items())\n",
    "        \n",
    "        del Xy\n",
    "        del y_train_pred\n",
    "        del y_test_pred\n",
    "        del X_train\n",
    "        del y_train\n",
    "        del X_test\n",
    "        del y_test\n",
    "        del train_index\n",
    "        del test_index\n",
    "        \n",
    "    return {item[0]: np.mean(item[1]) for item in train_errors.items()}, {item[0]: np.mean(item[1]) for item in test_errors.items()}\n",
    "\n",
    "def bias_variance(X, y, start, stop, step):\n",
    "    data_sizes = np.arange(start, stop, step)\n",
    "    train_errors = defaultdict(list)\n",
    "    test_errors = defaultdict(list)\n",
    "\n",
    "    for data_size in data_sizes:\n",
    "        print('Size %i' % data_size)\n",
    "        train_error, test_error = measure(data_size, X, y)\n",
    "\n",
    "        map(lambda item: train_errors[item[0]].append(item[1]), train_error.items())\n",
    "        map(lambda item: test_errors[item[0]].append(item[1]), test_error.items())\n",
    "    \n",
    "    return data_sizes, train_errors, test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run Profiler\n",
    "#import prof\n",
    "#reload(prof)\n",
    "#%lprun -T lprof -f prof.measure prof.measure(60, X, y)\n",
    "#%mprun -T mprof -f prof.measure prof.measure(100, X, y)\n",
    "#%mprun -T mprof -f prof.bias_variance prof.bias_variance(X, y, 100, 300, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sizes, train_errors, test_errors = bias_variance(X, y, 50, 500, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_bias_variance(data_sizes, train_errors, test_errors, tags):\n",
    "    plt.figure(num=None, figsize=(6, 5))\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('Data set size')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title(\"Bias-Variance\")\n",
    "    map(lambda tag: plt.plot(data_sizes, train_errors[tag], data_sizes, test_errors[tag], lw=1), tags)\n",
    "    plt.legend(list(chain.from_iterable([['%s test error' % tag, '%s train error' % tag] for tag in tags])), loc=\"upper right\")\n",
    "    plt.grid(True, linestyle='-', color='0.75')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_bias_variance(data_sizes,\n",
    "                   train_errors,\n",
    "                   test_errors,\n",
    "                   tags=['B-OCCATION', 'I-OCCATION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_bias_variance(data_sizes,\n",
    "                   train_errors,\n",
    "                   test_errors,\n",
    "                   tags=['B-LOCATION', 'I-LOCATION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_bias_variance(data_sizes,\n",
    "                   train_errors,\n",
    "                   test_errors,\n",
    "                   tags=['B-ADDRESS', 'I-ADDRESS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_bias_variance(data_sizes,\n",
    "                   train_errors,\n",
    "                   test_errors,\n",
    "                   tags=['B-DATE_PERIOD', 'I-DATE_PERIOD'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
