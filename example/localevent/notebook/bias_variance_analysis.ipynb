{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#%load_ext line_profiler\n",
    "#%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dic = corpora.Dictionary()\n",
    "tokens = ['']\n",
    "for line in open('data.conll', 'r'):\n",
    "    line = line.strip()\n",
    "    if len(line) == 0:\n",
    "        dic.add_documents([tokens])\n",
    "        del tokens[:]\n",
    "    else:\n",
    "        tokens.extend(line.split()[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sqlearn.crfsuite import crfutils\n",
    "from sqlearn.crfsuite import ner\n",
    "\n",
    "def load_data(f):\n",
    "    X = []\n",
    "    y = []\n",
    "    sent = []\n",
    "    sent_label = []\n",
    "    for line in codecs.open(f, 'r', 'utf-8'):\n",
    "        line = line.strip('\\n')\n",
    "        if line.strip() == '':\n",
    "            sent.append('\\n')\n",
    "            for item in crfutils.readiter(sent, ['w', 'pos'], ' ', dic.token2id):\n",
    "                ner.feature_extractor(item)\n",
    "                X.append(item)\n",
    "                y.append(sent_label)\n",
    "            sent = []\n",
    "            sent_label = []\n",
    "        else:\n",
    "            splited_line = line.split(' ')\n",
    "            sent.append('%s %s' % (splited_line[0], splited_line[1]))\n",
    "            sent_label.append(splited_line[2])\n",
    "\n",
    "    X = [[feature['F'] for feature in sent] for sent in X]\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X, y = load_data('data.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%writefile prof.py\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import pycrfsuite as crf\n",
    "\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "trainer = crf.Trainer(verbose=False)\n",
    "trainer.set_params({\n",
    "#    'c1': 0.0,   # coefficient for L1 penalty\n",
    "    'c2': 1.0,   # coefficient for L2 penalty\n",
    "#    'max_iterations': 50,  # stop earlier\n",
    "#\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "tagger = crf.Tagger()\n",
    "    \n",
    "def f_score(y_true, y_pred):\n",
    "    lb = LabelBinarizer()\n",
    "    y_true = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    y_acc = y_true + y_pred\n",
    "    \n",
    "#    print {item[0]: sum(y_true_combined[:, item[1]] > 0) for item in class_indices.items()}\n",
    "#    print {item[0]: sum(y_pred_combined[:, item[1]] > 0) for item in class_indices.items()}\n",
    "    \n",
    "    return {item[0]: \n",
    "            (f_acc(y_acc, item[1]),\n",
    "             f_rec(y_acc, y_true, item[1]),\n",
    "             f_pre(y_acc, y_pred, item[1]),\n",
    "             f_f1(y_acc, y_true, y_pred, item[1])) for item in class_indices.items()}\n",
    "\n",
    "def f_acc(y_acc, index):\n",
    "    return sum(y_acc[:, index] == 2)/sum(y_acc[:, index] > 0)\n",
    "\n",
    "def f_rec(y_acc, y_true, index):\n",
    "    return sum(y_acc[:, index] == 2)/sum(y_true[:, index] == 1)\n",
    "\n",
    "def f_pre(y_acc, y_pred, index):\n",
    "    return sum(y_acc[:, index] == 2)/sum(y_pred[:, index] == 1)\n",
    "\n",
    "def f_f1(y_acc, y_true, y_pred, index):\n",
    "    rec = f_rec(y_acc, y_true, index)\n",
    "    pre = f_pre(y_acc, y_pred, index)\n",
    "    return 2*rec*pre/(rec+pre)\n",
    "\n",
    "def measure(data_size, X, y):\n",
    "    # tag -> metrics -> [score0, scor1]\n",
    "    train_scores = defaultdict(lambda: defaultdict(list))\n",
    "    test_scores = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    def append_score(d, item):\n",
    "        d[item[0]]['acc'].append(item[1][0])\n",
    "        d[item[0]]['rec'].append(item[1][1])\n",
    "        d[item[0]]['pre'].append(item[1][2])\n",
    "        d[item[0]]['f1'].append(item[1][3])\n",
    "\n",
    "    kf = cross_validation.KFold(n=data_size, n_folds=3, shuffle=True, random_state=None)\n",
    "    \n",
    "    for fold_idx, (train_index, test_index) in enumerate(kf):\n",
    "        print('Iteration #%i' % fold_idx)\n",
    "        \n",
    "        X_train, y_train = X[train_index], y[train_index]\n",
    "        X_test, y_test = X[test_index], y[test_index]\n",
    "\n",
    "        # train\n",
    "        model_name = 'm%i.crfsuite' % fold_idx\n",
    "        Xy = zip(X_train, y_train)\n",
    "        for xseq, yseq in Xy:\n",
    "            trainer.append(xseq, yseq)\n",
    "        trainer.train(model_name)\n",
    "        trainer.clear()\n",
    "        \n",
    "        # predict\n",
    "        tagger.open(model_name)\n",
    "        y_train_pred = [tagger.tag(xseq) for xseq in X_train]\n",
    "        y_test_pred  = [tagger.tag(xseq) for xseq in X_test]\n",
    "        tagger.close()\n",
    "        \n",
    "        # evaluate\n",
    "        train_score = f_score(y_train, y_train_pred)\n",
    "        test_score = f_score(y_test, y_test_pred)\n",
    "        \n",
    "        map(lambda item: append_score(train_scores, item), train_score.items())\n",
    "        map(lambda item: append_score(test_scores, item), test_score.items())\n",
    "        \n",
    "        del Xy\n",
    "        del y_train_pred\n",
    "        del y_test_pred\n",
    "        del X_train\n",
    "        del y_train\n",
    "        del X_test\n",
    "        del y_test\n",
    "        del train_index\n",
    "        del test_index\n",
    "        \n",
    "    return ({item[0]: (np.mean(item[1]['acc']), np.mean(item[1]['rec']), np.mean(item[1]['pre']), np.mean(item[1]['f1'])) for item in train_scores.items()},\n",
    "            {item[0]: (np.mean(item[1]['acc']), np.mean(item[1]['rec']), np.mean(item[1]['pre']), np.mean(item[1]['f1'])) for item in test_scores.items()})\n",
    "\n",
    "def bias_variance(X, y, start, stop, step):\n",
    "    data_sizes = np.arange(start, stop, step)\n",
    "    train_scores = defaultdict(lambda: defaultdict(list))\n",
    "    test_scores = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    def append_score(d, item):\n",
    "        d[item[0]]['Error'].append(1-item[1][0])\n",
    "        d[item[0]]['Recall'].append(item[1][1])\n",
    "        d[item[0]]['Precision'].append(item[1][2])\n",
    "        d[item[0]]['F1'].append(item[1][3])\n",
    "        \n",
    "    for data_size in data_sizes:\n",
    "        print('Size %i' % data_size)\n",
    "        train_score, test_score = measure(data_size, X, y)\n",
    "        \n",
    "        map(lambda item: append_score(train_scores, item), train_score.items())\n",
    "        map(lambda item: append_score(test_scores, item), test_score.items())\n",
    "    \n",
    "    return data_sizes, train_scores, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run Profiler\n",
    "#import prof\n",
    "#reload(prof)\n",
    "#%lprun -T lprof -f prof.measure prof.measure(60, X, y)\n",
    "#%mprun -T mprof -f prof.measure prof.measure(100, X, y)\n",
    "#%mprun -T mprof -f prof.bias_variance prof.bias_variance(X, y, 100, 300, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sizes, train_scores, test_scores = bias_variance(X, y, 50, 500, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_bias_variance(data_sizes, train_scores, test_scores, tags):\n",
    "    plt.figure(num=None, figsize=(6, 5))\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('Data set size')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title(\"Bias-Variance\")\n",
    "    map(lambda tag: plt.plot(data_sizes, train_scores[tag]['Error'], data_sizes, test_scores[tag]['Error'], lw=1), tags)\n",
    "    plt.legend(list(chain.from_iterable([['%s train error' % tag, '%s test error' % tag] for tag in tags])), loc=\"upper right\")\n",
    "    plt.grid(True, linestyle='-', color='0.75')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_score(data_sizes, scores, score_name, tags):\n",
    "    plt.figure(num=None, figsize=(6, 5))\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('Data set size')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(score_name)\n",
    "    map(lambda tag: plt.plot(data_sizes, scores[tag][score_name], lw=1), tags)\n",
    "    plt.legend(tags, loc=\"upper right\")\n",
    "    plt.grid(True, linestyle='-', color='0.75')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_bias_variance(data_sizes,\n",
    "                   train_scores,\n",
    "                   test_scores,\n",
    "                   tags=['B-OCCATION', 'I-OCCATION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_score(data_sizes,\n",
    "           test_scores,\n",
    "           'F1',\n",
    "           tags=['B-OCCATION', 'I-OCCATION'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
